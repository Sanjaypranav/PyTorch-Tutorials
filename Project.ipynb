{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565ea750",
   "metadata": {
    "id": "565ea750"
   },
   "source": [
    "## Project Introduction\n",
    "\n",
    "Build a simple convolutional neural network in PyTorch and Train it to recognize natural objects using the CIFAR-10 dataset. The structure:\n",
    "\n",
    "    1. Introduction\n",
    "    2. Setting up the Environment\n",
    "    3. Preparing the Data\n",
    "    4. Building the Network\n",
    "    5. Training the Model and Evaluating the Performance\n",
    "    6. Understand the Deep Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba50750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21e4983f",
   "metadata": {
    "id": "21e4983f"
   },
   "source": [
    "### Setting up the Environment\n",
    "\n",
    "```\n",
    "!pip install torch torchvision\n",
    "!pip install Pillow==4.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc18d572",
   "metadata": {
    "collapsed": true,
    "id": "fc18d572",
    "outputId": "d325191a-e1ba-4a2b-e618-f427dd5a7890"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports numpy, torch, torchvision, matplotlib\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import math\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317f930",
   "metadata": {
    "id": "9317f930"
   },
   "source": [
    "### Some testing for Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "867a4202",
   "metadata": {
    "id": "867a4202",
    "outputId": "c0817094-b0e1-4ec2-8ec1-bdb64e74cecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 2])\n",
      "cpu\n",
      "tensor([20., 30., 50.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#some testing here\n",
    "\n",
    "# Operations\n",
    "y = torch.rand(2, 2)\n",
    "x = torch.rand(2, 2)\n",
    "\n",
    "# multiplication\n",
    "z = x * y\n",
    "z = torch.mul(x,y) #elementwise #must be broadcastable\n",
    "\n",
    "#matrix multiplication\n",
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(4, 3)\n",
    "torch.matmul(tensor1, tensor2)\n",
    "\n",
    "# NumPy conversion\n",
    "x = torch.rand(2,2)\n",
    "y = x.numpy()\n",
    "print(type(y))\n",
    "\n",
    "z1 = torch.from_numpy(y) #sharing the memory space with the numpy ndarray\n",
    "z2 = torch.tensor(y) #a copy\n",
    "print(type(z1))\n",
    "print(type(z2))\n",
    "\n",
    "# Pytorch attributes and functions for tensors\n",
    "print(x.shape)\n",
    "print(x.device)\n",
    "\n",
    "# autograd\n",
    "\n",
    "# requires grad equals true lets us compute gradients on the tenor\n",
    "x = torch.tensor([2,3,5], dtype=float, requires_grad=True)\n",
    "y =(5 * x**2).sum()\n",
    "\n",
    "# When computation is finished .backward() and have all the gradients computed automatically\n",
    "# The gradient for this tensor will be accumulated into .grad attribute\n",
    "y.backward()\n",
    "#print(z.grad) # dz/dz\n",
    "print(x.grad) # dz/dx\n",
    "\n",
    "# autograd requires computational resources and can take time.\n",
    "# disable autograd for model eval by writing your evaluation code in \n",
    "# As such, with torch.no_grad() is usually used in evaluation part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df4c6f",
   "metadata": {
    "id": "d9df4c6f"
   },
   "source": [
    "For repeatable experiments, I recommended to set random seeds for anything using random number generation - this means numpy and random as well! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67005511",
   "metadata": {
    "id": "67005511"
   },
   "outputs": [],
   "source": [
    "experiment_name = 'debug'  #Provide name to model experiment\n",
    "model_name = 'basic' # Choose between [basic, alexnet]\n",
    "batch_size = 5  #You may not need to change this but incase you do\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da43928",
   "metadata": {
    "id": "2da43928"
   },
   "source": [
    "## Preparing the Data\n",
    "\n",
    "I have worte some data-loading functions and a few helper functions below. \n",
    "\n",
    "To note, the effects to be added on the training data should also consider the property of the dataset. For example, for the digit recognition in MNIST, can we perform 180-degree rotation on the image of digit 6 without altering the image label during network training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8b58b",
   "metadata": {
    "id": "dca8b58b"
   },
   "outputs": [],
   "source": [
    "def get_transform(model_name):\n",
    "\n",
    "    if model_name == 'alexnet':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((227, 227)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "\n",
    "def get_dataset(model_name, train_percent=0.9):\n",
    "    '''\n",
    "    Returns the train, val and test torch.Datasetin addition to a list of classes, where the idx of class name corresponds\n",
    "    to the label used for it in the data\n",
    "    \n",
    "    \n",
    "    @model_name: either 'basic' or 'alexnet'\n",
    "    @train_percent: percent of training data to keep for training. Rest will be validation.\n",
    "    '''\n",
    "    \n",
    "    transform  = get_transform(model_name)\n",
    "    \n",
    "    train_data = CIFAR10(root='./data', train=True, download=first_run, transform=transform)\n",
    "    test_data  = CIFAR10(root='./data', train=False, download=first_run, transform=transform)\n",
    "\n",
    "    train_size = int(train_percent * len(train_data))\n",
    "    val_size = len(train_data) - train_size\n",
    "\n",
    "\n",
    "    train_data, val_data = random_split(train_data, [train_size, val_size])\n",
    "    class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    return train_data, val_data, test_data, class_names\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size, num_workers=1, model_name='basic'):\n",
    "    '''\n",
    "    Returns the train, val and test dataloaders in addition to a list of classes, where the idx of class name corresponds\n",
    "    to the label used for it in the data\n",
    "    \n",
    "    Reference for dataloader class: https://pytorch.org/docs/stable/data.html\n",
    "    @batch_size: batch to be used by dataloader\n",
    "    @num_workers: number of dataloader workers/instances used\n",
    "    @model_name: either 'basic' or 'alexnet'\n",
    "    '''\n",
    "    \n",
    "    train_set, val_set, test_set, class_names = get_dataset(model_name)\n",
    "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    valloader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    testloader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    \n",
    "    return trainloader, valloader, testloader, class_names\n",
    "\n",
    "def makegrid_images(model_name='basic'):\n",
    "    '''\n",
    "    For visualization purposes\n",
    "    \n",
    "    @model_name: either 'basic' or 'alexnet'\n",
    "    '''\n",
    "    \n",
    "\n",
    "    _, trial_loader, _, _ = get_dataloader(32, model_name=model_name)\n",
    "    images, labels = iter(trial_loader).next()\n",
    "    \n",
    "    grid = make_grid(images)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def show_img(img, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), viz=True, norm=True):\n",
    "    '''\n",
    "    For visualization purposes\n",
    "    \n",
    "    B: batch size\n",
    "    C: channels\n",
    "    H: height\n",
    "    W: width\n",
    "    \n",
    "    @img: torch.Tensor for the image of type (B, C, H, W) \n",
    "    @mean: mean used for normalizing along 3 dimensions C, H, W in get_transform\n",
    "    @std:  std. deviation used for normalizing along 3 dimensions C, H, WW in get_transform\n",
    "    @viz: whether or not to plt.plot or just return the unnormalized image\n",
    "    @norm: whether or not unnormalize. Unnormalizes if true.\n",
    "    \n",
    "    Returns:\n",
    "    Viewable image in (H, W, C) as a numpy array\n",
    "    '''\n",
    "    \n",
    "\n",
    "    if norm:\n",
    "        for idx in range(img.shape[0]):\n",
    "\n",
    "            img[idx] = img[idx] * std[idx] + mean[idx]\n",
    "        \n",
    "    image = np.asarray(img)\n",
    "\n",
    "    if viz:\n",
    "        if len(image.shape) == 4:\n",
    "            image = image.squeeze()\n",
    "\n",
    "        plt.imshow(np.transpose(image, (1, 2, 0)))\n",
    "        plt.show()\n",
    "\n",
    "    return np.transpose(image.squeeze(), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65337e5",
   "metadata": {
    "id": "e65337e5"
   },
   "source": [
    "## Building the Network\n",
    "\n",
    "### Training from scratch\n",
    "\n",
    "First, below will train a shallow convolutional neural network. Neural network is defined in **GradBasicNet**, randomly initialize the value of the parameters in the network, and train it on the training dataset:\n",
    "\n",
    "1. Fill out **get_conv_layers()** with a network of **2 conv layers** each with kernel size of 5. After the first convolutional layers, add a **max pooling layer** with kernel size of 2 and stride of 2. Remember to add the non linearities immediately after the conv layers. You must choose the in-channels and out-channels for both the layers: the image to be input will be RGB so there is only one number that can be used for the in-channels of the first conv layer. Use the nn.Sequential API to combine all this into one layer, and return from **get_conv_layers()** method\n",
    "2. Fill out **final_pool_layer()** with a max pooling layer. Typically after all the convolutional layers there is another max pooling layer. Use the same kernel size and stride as before and this time directly return the **nn.MaxPool2d**. \n",
    "3. Fill out the **get_fc_layers()** method with a classifier containing 3 linear layers. Feel free to choose the in_channels and out_channels for these. Once again use the **nn.Sequential** API and return the object from the method. Inside, alternate the Linear layers with ReLU activations. You do not need a ReLU after the final layer. Remember that the first Linear layer must take in the output of the final convolution layer so depending on the choice in (1.) there is only 1 value which can have for the in_channels of the first Linear layer. Also the final Linear layer must have out_channels=10 since performing 10-way classification. Lastly, **remember to add comments** on why you keep the relu of the first two layers while remove the relu of the last layer.\n",
    "4. Finally, please fill out the forward pass using these layers. Remember to use **self.conv_model**, **self.final_max_pool** and **self.fc_model** one after the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86eb1a",
   "metadata": {
    "id": "bd86eb1a"
   },
   "outputs": [],
   "source": [
    "class GradBasicNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_model = self.get_conv_layers()\n",
    "        self.final_max_pool = self.final_pool_layer()\n",
    "        self.fc_model = self.get_fc_layers()\n",
    "\n",
    "    def get_conv_layers(self):\n",
    "\n",
    "        #TODO: Group the convolutional layers using nn.Sequential\n",
    "        \n",
    "        layers = \n",
    "\n",
    "        return layers\n",
    "\n",
    "    def final_pool_layer(self):\n",
    "\n",
    "        #TODO Set this to a MaxPool layers\n",
    "        \n",
    "        layer = \n",
    "        \n",
    "        return layer\n",
    "\n",
    "    def get_fc_layers(self):\n",
    "\n",
    "        # TODO Group the linear layers using nn.Sequential\n",
    "        \n",
    "        layers = \n",
    "        \n",
    "        # =================================================\n",
    "        # Please add the comment here(detail in above description)\n",
    "        # =================================================\n",
    "        return layers\n",
    "\n",
    "    def register_grad_hook(self, grad):\n",
    "        self.grad = grad\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #TODO\n",
    "        x = #call the conv layers \n",
    "        #ignore this: relevance for gradcam section\n",
    "        h = x.register_hook(self.register_grad_hook)\n",
    "\n",
    "        x = #call the max pool layer\n",
    "        x = #flatten the output of x \n",
    "        x = #call the fully connected layers\n",
    "\n",
    "    def get_gradient_activations(self):\n",
    "        return self.grad\n",
    "\n",
    "    def get_final_conv_layer(self, x):\n",
    "        return self.conv_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ced008",
   "metadata": {
    "id": "87ced008"
   },
   "source": [
    "### Finetuning on a pre-trained model\n",
    "\n",
    "Secondly, below will a stronger convolutional neural network by starting from a pre-trained model **AlexNet**:\n",
    "\n",
    "In this subsection, might have to take advantage of a pretrained network in a process called transfer learning, where only train a few final layers of a neural network. Here use the AlexNet architecture that revolutionized Deep Learning. The pretrained model (on ImageNet) is available on torchvision library and ask PyTorch to allow updates on a few of thee final layers during training. Put the AlexNet model also into the API that's used for the model allow, except that needing an additionl **transition layer** function for the added **AvgPool** layer. Visualize the model by running the code cell. \n",
    "1. (features) contains most of the conv layers. We need upto (11) to include every conv layer\n",
    "2. (features) (12) is the final max pooling layer\n",
    "3. (avgpool) is the transition average pooling layer\n",
    "4. (classifier) is the collection of linear layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4edb7",
   "metadata": {
    "id": "78e4edb7"
   },
   "outputs": [],
   "source": [
    "example_model = models.alexnet(pretrained=True)\n",
    "print(example_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c5390",
   "metadata": {
    "id": "8f1c5390"
   },
   "source": [
    "TODO task is two fold:\n",
    "\n",
    "1. Implement the method **activate training layers** which sets the requires_grad of relevant parameters to True, so that training can occur, which can iterate over training parameters with \n",
    "\n",
    "``\n",
    "for name, param in self.conv_model.named_parameters():\n",
    "``\n",
    "\n",
    "and \n",
    "\n",
    "``\n",
    "for name, param in self.fc_model.named_parameters():\n",
    "``\n",
    "\n",
    "For the conv layers, every param should have requires_grad set to false except for the last layer (10). \n",
    "For the linear layers, all layers must be trainable aka requires_grad must be set to True.\n",
    "\n",
    "2. Implement the forward pass in the following order: self.conv_model, self.final_max_pool, self.avg_pool, self.fc_model\n",
    "\n",
    "3. Remember to fill in the comments under the question mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb97ce",
   "metadata": {
    "id": "63bb97ce"
   },
   "outputs": [],
   "source": [
    "class GradAlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_alex_net = models.alexnet(pretrained=True)\n",
    "\n",
    "\n",
    "        self.conv_model = self.get_conv_layers()\n",
    "        self.final_max_pool = self.final_pool_layer()\n",
    "        self.avg_pool = self.transition_layer()\n",
    "        self.fc_model = self.get_fc_layers()\n",
    "\n",
    "        self.activate_training_layers()\n",
    "\n",
    "    def activate_training_layers(self):\n",
    "\n",
    "        #TODO Fill out the function below\n",
    "        for name, param in self.conv_model.named_parameters():\n",
    "            \n",
    "            print(name)\n",
    "            #this is the number of every convolutional layer. From what model printed above, what is\n",
    "            #the last convolutional layer?\n",
    "            # =================================================\n",
    "            # Please comment here\n",
    "            # =================================================\n",
    "            number = int(name.split('.')[0])\n",
    "            \n",
    "            # TODO: for all layers except the last layer set param.requires_grad = False\n",
    "\n",
    "        for name, param in self.fc_model.named_parameters():\n",
    "\n",
    "            # for all of these layers set param.requires_grad as True\n",
    "\n",
    "\n",
    "    def get_conv_layers(self):\n",
    "\n",
    "        return self.base_alex_net.features[:12]\n",
    "\n",
    "    def final_pool_layer(self):\n",
    "        return nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "    def transition_layer(self):\n",
    "        return nn.AdaptiveAvgPool2d(output_size=(6, 6))\n",
    "\n",
    "    def get_fc_layers(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=9216, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=4096, out_features=1000, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=1000, out_features=10, bias=True),\n",
    "        )\n",
    "\n",
    "    def register_grad_hook(self, grad):\n",
    "        self.grad = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #TODO fill out the forward pass\n",
    "        x = #call the conv layers\n",
    "        h = x.register_hook(self.register_grad_hook)\n",
    "\n",
    "        x = #call the max pool layer\n",
    "        x = #call the avg pool layer\n",
    "        x = #call fully connected layers\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def get_gradient_activations(self):\n",
    "        return self.grad\n",
    "\n",
    "    def get_final_conv_layer(self, x):\n",
    "        return self.conv_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9146e92",
   "metadata": {
    "id": "d9146e92"
   },
   "source": [
    "#### Please add a comment:\n",
    "After finish the implementation and run the experiments, please add a comment explaining questions below:\n",
    "1. what is the main difference between finetuning and training from scratch.\n",
    "\n",
    "2. Compare the performance between finetuning from a pre-trained model and training from scratch. Explain why the better one outperforms the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa68d17",
   "metadata": {
    "id": "4aa68d17"
   },
   "source": [
    "## Training the Model and Evaluating the Model's Performance\n",
    "\n",
    "To note, one training loop is denoted as one epoch and means all of the training data has been used to train the deep neural network once. Please fill out the classifier we use to train these networks. \n",
    "1. In the **__init__** method fill out the **self.criterion** and **self.optimizer**. Remember this is a classification problem so will need a cross entropy loss. For the optimizer, recommending using stochastic gradient descent with a learning rate of 0.001 and momentum of 0.9. The rest of **__init__** has been filled out.\n",
    "2. Next, fill out the **training loop**. Expected to iterate over **self.dataloaders['train']** and optimize on the loss with the groundtruth labels. The **validation** aspect of the training loop has been provided and so has the evaluate method. In the training loop, print the average loss every 1000 images processed. Remember to zero out gradients on the model before doing loss.backward(), and then only after this backward step, make a step in the right direction using the optimizer.\n",
    "\n",
    "At this stage ignore all methods, after evaluate. They will be relevant to later sections and you will have to return to them when you have more instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8f4b2",
   "metadata": {
    "id": "58a8f4b2"
   },
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "\n",
    "    def __init__(self, name, model, dataloaders, class_names, use_cuda=False):\n",
    "        \n",
    "        '''\n",
    "        @name: Experiment name. Will define stored results etc. \n",
    "        @model: Either a GradBasicNet() or a GradAlexNet()\n",
    "        @dataloaders: Dictionary with keys train, val and test and corresponding dataloaders\n",
    "        @class_names: list of classes, where the idx of class name corresponds to the label used for it in the data\n",
    "        @use_cuda: whether or not to use cuda\n",
    "        '''\n",
    "        \n",
    "        self.name = name\n",
    "        if use_cuda and not torch.cuda.is_available():\n",
    "            raise Exception(\"Asked for CUDA but GPU not found\")\n",
    "            \n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.model = model.to('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "        #TODO\n",
    "        self.criterion = #use cross entropy loss\n",
    "        self.optim = #use SGD with suggest hyperparams; you must select all the model params\n",
    "\n",
    "        self.dataloaders = dataloaders\n",
    "        self.class_names = class_names\n",
    "        self.activations_path = os.path.join('activations', self.name)\n",
    "        self.kernel_path = os.path.join('kernel_viz', self.name)\n",
    "\n",
    "        save_path = os.path.join(os.getcwd(), 'models', self.name)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        if not os.path.exists(self.activations_path):\n",
    "            os.makedirs(self.activations_path)\n",
    "\n",
    "        if not os.path.exists(self.kernel_path):\n",
    "            os.makedirs(self.kernel_path)\n",
    "            \n",
    "        self.save_path = save_path\n",
    "\n",
    "    def train(self, epochs, save=True):\n",
    "        '''\n",
    "        @epochs: number of epochs to train\n",
    "        @save: whether or not to save the checkpoints\n",
    "        '''\n",
    "\n",
    "        best_val_accuracy = - math.inf\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            batches_in_pass = len(self.dataloaders['train'])\n",
    "            \n",
    "            #You may comment these two lines if you do not wish to use them\n",
    "            loss_total = 0.0 # Record the total loss within a few steps\n",
    "            epoch_loss = 0.0 # Record the total loss for each epoch\n",
    "            \n",
    "            # TODO Iterate over the training dataloader (see how it is done for validation below) and make sure\n",
    "            # to call the optim.zero_grad(), loss.backward() and optim.step()\n",
    "\n",
    "            '''Give validation'''\n",
    "            epoch_loss /= batches_in_pass\n",
    "\n",
    "            self.model.eval()\n",
    "            \n",
    "            #DO NOT modify this part\n",
    "            correct = 0.0\n",
    "            total = 0.0\n",
    "            for idx, data in enumerate(self.dataloaders['val']):\n",
    "\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to('cuda' if self.use_cuda else 'cpu')\n",
    "                labels = labels.to('cuda' if self.use_cuda else 'cpu')\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                total += labels.shape[0]\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_accuracy = 100 * correct / total\n",
    "\n",
    "            print(f'Train Epoch Loss (Avg): {epoch_loss}')\n",
    "            print(f'Validation Epoch Accuracy:{epoch_accuracy}')\n",
    "            \n",
    "            if save:\n",
    "                #  Make sure that your saving pipeline is working well. \n",
    "                # Is os library working on your file system? \n",
    "                # Is your model being saved and reloaded fine? \n",
    "                # When you do the kernel viz, activation maps, \n",
    "                # and GradCAM you must be using the model you have saved before.\n",
    "                \n",
    "                torch.save(self.model.state_dict(), os.path.join(self.save_path, f'epoch_{epoch}.pt'))\n",
    "                \n",
    "                if epoch_accuracy > best_val_accuracy:\n",
    "\n",
    "                    torch.save(self.model.state_dict(), os.path.join(self.save_path, 'best.pt'))\n",
    "                    best_val_accuracy = epoch_accuracy\n",
    "\n",
    "        print('Done training!')                       \n",
    "\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \n",
    "        try:\n",
    "            assert os.path.exists(os.path.join(self.save_path, 'best.pt'))\n",
    "            \n",
    "        except:\n",
    "            print('It appears you are testing the model without training. Please train first')\n",
    "            return\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(os.path.join(self.save_path, 'best.pt')))\n",
    "        self.model.eval()\n",
    "\n",
    "        #total = len(self.dataloaders['test'])\n",
    "        \n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for idx, data in enumerate(self.dataloaders['test']):\n",
    "            \n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to('cuda' if self.use_cuda else 'cpu')\n",
    "                labels = labels.to('cuda' if self.use_cuda else 'cpu')\n",
    "                \n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                total += labels.shape[0]\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        print(f'Accuracy: {100 * correct/total}%')\n",
    "        \n",
    "    def grad_cam_on_input(self, img):\n",
    "        \n",
    "        try:\n",
    "            assert os.path.exists(os.path.join(self.save_path, 'best.pt'))\n",
    "\n",
    "        except:\n",
    "            print('It appears you are testing the model without training. Please train first')\n",
    "            return\n",
    "\n",
    "        self.model.load_state_dict(torch.load(os.path.join(self.save_path, 'best.pt')))\n",
    "\n",
    "\n",
    "        self.model.eval()\n",
    "        img = img.to('cuda' if self.use_cuda else 'cpu')\n",
    "\n",
    "\n",
    "        out = self.model(img)\n",
    "\n",
    "        _, pred = torch.max(out, 1)\n",
    "\n",
    "        predicted_class = self.class_names[int(pred)]\n",
    "        print(f'Predicted class was {predicted_class}')\n",
    "\n",
    "        out[:, pred].backward()\n",
    "        gradients = self.model.get_gradient_activations()\n",
    "\n",
    "        print('Gradients shape: ', f'{gradients.shape}')\n",
    "\n",
    "        mean_gradients = torch.mean(gradients, [0, 2, 3]).cpu()\n",
    "        activations = self.model.get_final_conv_layer(img).detach().cpu()\n",
    "\n",
    "        print('Activations shape: ', f'{activations.shape}')\n",
    "\n",
    "        for idx in range(activations.shape[1]):\n",
    "            activations[:, idx, :, :] *= mean_gradients[idx]\n",
    "\n",
    "        final_heatmap = np.maximum(torch.mean(activations, dim=1).squeeze(), 0)\n",
    "\n",
    "        final_heatmap /= torch.max(final_heatmap)\n",
    "\n",
    "        return final_heatmap\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def trained_kernel_viz(self):\n",
    "        \n",
    "        all_layers = [0, 3]\n",
    "        all_filters = []\n",
    "        for layer in all_layers:\n",
    "\n",
    "            #TODO: blank out first line\n",
    "            filters = self.model.conv_model[layer].weight\n",
    "            all_filters.append(filters.detach().cpu().clone()[:8, :8, :, :])\n",
    "\n",
    "        for filter_idx in range(len(all_filters)):\n",
    "\n",
    "            filter = all_filters[filter_idx]\n",
    "            print(filter.shape)\n",
    "            filter = filter.contiguous().view(-1, 1, filter.shape[2], filter.shape[3])\n",
    "            image = show_img(make_grid(filter))\n",
    "            image = 255 * image\n",
    "            cv2.imwrite(os.path.join(self.kernel_path, f'filter_layer{all_layers[filter_idx]}.jpg'), image)\n",
    "    \n",
    "\n",
    "    def activations_on_input(self, img):\n",
    "        \n",
    "        img = img.to('cuda' if self.use_cuda else 'cpu')\n",
    "\n",
    "        all_layers = [0,3,6,8,10]\n",
    "        all_viz = []\n",
    "\n",
    "        for each in all_layers:\n",
    "\n",
    "            current_model = self.model.conv_model[:each+1]\n",
    "            current_out = current_model(img)\n",
    "            all_viz.append(current_out.detach().cpu().clone()[:, :64, :, :])\n",
    "\n",
    "        for viz_idx in range(len(all_viz)):\n",
    "\n",
    "            viz = all_viz[viz_idx]\n",
    "            viz = viz.view(-1, 1, viz.shape[2], viz.shape[3])\n",
    "            image = show_img(make_grid(viz))\n",
    "            image = 255 * image\n",
    "            cv2.imwrite(os.path.join(self.activations_path, f'sample_layer{all_layers[viz_idx]}.jpg'), image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9167e2d",
   "metadata": {
    "id": "f9167e2d"
   },
   "source": [
    "Run the classifier for the code using the basic model by running the following snippets. If all goes well, should have a test accuracy of about ~60-70% at the end of it. It should take <10 mins to run on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b19f80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "12405b11f78b4f4bb19db5102461bb13",
      "d02d457af42f4303b8ab195eacb434f1",
      "90d3fd78a8ab47d49d861f1760be3a3e",
      "fea008a7bd2b4372a6a733e1a6530799",
      "1de732a5f6554fd7a5262acadbf6d398",
      "7aed49d62b234bd29a47048ca82c6c86",
      "44b96e0dc09a4db98e01f9a38da9ea76",
      "ce3eb6de33a944c7897092304ef2d1ef",
      "2d04f101ec9546afad0efd05d85a17f9",
      "7e9a25d7d7d94353b19a34acf95d3bbc",
      "b04888374a304257b9fa11f6689497a5",
      "ecf06578e684462486d1c84f1600b8b6"
     ]
    },
    "id": "a3b19f80",
    "outputId": "7165e93f-ec2a-489b-b08f-050bf464e113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf06578e684462486d1c84f1600b8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_name = 'basic_debug'  #Provide name to model experiment\n",
    "model_name = 'basic' #Choose between [basic, alexnet]\n",
    "batch_size = 5  #You may not need to change this but incase you do\n",
    "first_run = True #whether or not first time running it\n",
    "\n",
    "trainloader, valloader, testloader, class_names = get_dataloader(batch_size=batch_size, model_name=model_name)\n",
    "dataloaders = {'train': trainloader, 'val' : valloader, 'test': testloader, 'mapping': class_names}\n",
    "\n",
    "if model_name == 'basic':\n",
    "    model = GradBasicNet()\n",
    "elif model_name == 'alexnet':\n",
    "    model = GradAlexNet()\n",
    "else:\n",
    "    raise NotImplementedError(\"This option has not been implemented. Choose between 'basic' and 'alexnet' \")\n",
    "\n",
    "classifier = Classifier(experiment_name, model, dataloaders, class_names, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77206a9",
   "metadata": {
    "id": "b77206a9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# When you develop your code, to save your time, you can choose to run the model\n",
    "# for 5 cpoches. The accuracy after training for 5 epoches has already been high\n",
    "# and close to the model after 20-epoch training.\n",
    "# (As a reference, Validation Epoch Accuracy is above 61 after training 5 epoches)\n",
    "\n",
    "# To note, for your final submission, make sure to train the model \n",
    "# for 20 epoches and analysis that model in the later sections.\n",
    "\n",
    "# classifier.train(epochs=5) # For your reference\n",
    "classifier.train(epochs=20)\n",
    "classifier.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f2246",
   "metadata": {
    "id": "407f2246"
   },
   "source": [
    "Now run the classifier for the code using the alexnet model specified above, which will have a notable performance increase. On a GPU, this trained for about 30 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bac720",
   "metadata": {
    "id": "86bac720"
   },
   "outputs": [],
   "source": [
    "experiment_name = 'alexnet_debug'  #Provide name to model experiment\n",
    "model_name = 'alexnet' #Choose between [basic, alexnet]\n",
    "batch_size = 5  #may not need to change this but incase you do\n",
    "first_run = True #whether or not first time running it\n",
    "\n",
    "trainloader, valloader, testloader, class_names = get_dataloader(batch_size=batch_size, model_name=model_name)\n",
    "dataloaders = {'train': trainloader, 'val' : valloader, 'test': testloader, 'mapping': class_names}\n",
    "\n",
    "#model = models.alexnet(pretrained=True)\n",
    "if model_name == 'basic':\n",
    "\n",
    "    model = GradBasicNet()\n",
    "\n",
    "elif model_name == 'alexnet':\n",
    "\n",
    "    model = GradAlexNet()\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(\"This option has not been implemented. Choose between 'basic' and 'alexnet' \")\n",
    "\n",
    "classifier = Classifier(experiment_name, model, dataloaders, class_names, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056d35c",
   "metadata": {
    "id": "9056d35c"
   },
   "outputs": [],
   "source": [
    "# When you develop your code, to save your time, you can choose to run the model\n",
    "# for 3 cpoches. The accuracy after training for 3 epoches has already been high\n",
    "# and close to the model after 20-epoch training.\n",
    "\n",
    "# To note, for your final submission, it is recommended to run the model for 20 epochs.\n",
    "# However, if it takes time, you should at least run the model for 5 epochs. \n",
    "\n",
    "# classifier.train(epochs=3) # For your reference\n",
    "classifier.train(epochs=20)\n",
    "classifier.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5974f5",
   "metadata": {
    "id": "2d5974f5"
   },
   "source": [
    "**Before you move forward to the next step**: Try and see what classes your model does well on (you can modify the testing code for this). This will help you pick the best visualization to show later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d684c47",
   "metadata": {
    "id": "0d684c47"
   },
   "source": [
    "## Deep Neural Networks\n",
    "\n",
    "### Activation Map\n",
    "\n",
    "For interpreting the alexnet model, which will be using a simple version of Grad-CAM (Gradient based Class activation mapping). (https://arxiv.org/abs/1610.02391). This will help to see what region of the input image the output is 'focusing on' while making its key choice. Below is the steps:\n",
    "\n",
    "1. First, move the img to cuda if you are using GPU.\n",
    "2. The code to load the model trained from before has been provided. Use self.model to output the predictions to the variable out. Your output should have dimension (1, 10). \n",
    "3. The predicted class is the index of the highest value of of these 10 values. Use **torch.max** along dim 1 to get the argument of the max value. This method will return two values, the latter of them is the required argument. The next two lines have been provided: they indicate the predicted loss\n",
    "4. Call the **backward** method on out[:, pred]. Previously during the forward passes, we have applied a gradient hook on the last convolutional layer which mean that during the applied backward pass, we will record the value of the gradient for the **maximum predicted class** with respect to the **output** of the last convolutional layer. This will be the same size as the **output** of the last convolutional layer. \n",
    "5. After the **backward** call, get the value of the gradient above using the **get_gradient_activations** function on **self.model** and store it in gradients. Now use the torch.mean method to get the mean value of gradients across all dimensionss except the channels dimension (number of filters) and store this in **mean_gradients**. The output should be of shape (1, 64, 1, 1). If the tensors have been on GPU, you should move them to CPU using .detach().cpu() \n",
    "6. Use the self.model.get_final_conv_layer(img) to store the activations at the final layer to activations. This should be of shape (1, 64, H, W). \n",
    "7. Now for each of the 64 filters, **scale** the activations at that filter, with the corresponding **mean_gradients** value for that filter. The output should have size (1, 64, H, W) Iterate over the 64 filters in the following way:\n",
    " ``\n",
    " for idx in range(activations.shape[1]):\n",
    " ``\n",
    "8. As a final step, take mean across the 64 filters and do a ReLU to get rid of negative activations before normalzing one last time to get the heatmap, which has been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b6e47",
   "metadata": {
    "id": "b50b6e47"
   },
   "outputs": [],
   "source": [
    "'''Sample an image from the test set'''\n",
    "\n",
    "#You may change the sampling code to sample an image as you desire.\n",
    "#Make sure to NOT move the sampling code to a different cell.\n",
    "\n",
    "img_batch, labels_batch = next(iter(testloader))\n",
    "img = img_batch[3]\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "classifier = Classifier(experiment_name, model, dataloaders, class_names, use_cuda=True)\n",
    "heatmap = classifier.grad_cam_on_input(img)\n",
    "\n",
    "def visualize(img, heatmap):\n",
    "\n",
    "\n",
    "    heatmap = heatmap.cpu().numpy()\n",
    "\n",
    "    img = show_img(img)\n",
    "    img = np.uint8(255 * img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    print(img.shape)\n",
    "    print(heatmap.shape)\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    combine = 0.5 * heatmap + img\n",
    "\n",
    "    #if not os.path.exists(write_path):\n",
    "    #    os.makedirs(write_path)\n",
    "\n",
    "    plt.imshow(combine/255)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize(img, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2848d5",
   "metadata": {
    "id": "2f2848d5"
   },
   "source": [
    "Show an example of an image where the method is looking at the object in question and another where it appear to be completely unrelated. In the latter case, it might have learnt a spurious correlation- or a bias in the data which always appears to be correlated with a given label. For the ship class, this **might** be the surrounding water or for a **horse** it might be the surrounding grass. In such cases, do you think the model would predict correctly for a ship on sand or a horse in the air? Please leave a comment in a text snippet below. **Causally trained neural networks** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83962e",
   "metadata": {
    "id": "0a83962e"
   },
   "outputs": [],
   "source": [
    "# Your code here to show an failure case. \n",
    "# You can refer the steps and functions implemented in the previous cell and reuse them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d07051",
   "metadata": {
    "id": "b9d07051"
   },
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ff9dd",
   "metadata": {
    "id": "a49ff9dd"
   },
   "source": [
    "### Kernel and Activation Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d33be0",
   "metadata": {
    "id": "d8d33be0"
   },
   "source": [
    "Visualize some learned convolutional kernels for two layers in the conv-net. Study the code provided for **trained_kernel_viz** carefully. Only have to fill out the line for **filter**. Expected to do is to access the relevantt layer from **self.conv_model** and set **filter** equal to its weight parameter.\n",
    "\n",
    "Call this function on the alexnet classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d37569",
   "metadata": {
    "id": "05d37569"
   },
   "outputs": [],
   "source": [
    "classifier.trained_kernel_viz()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b261d",
   "metadata": {
    "id": "2d3b261d"
   },
   "source": [
    "Now with the kernel viz filled out, write the method for activation visualizations **activations on input**. The structure for the code is very similar to the kernel viz, except that actually viewing the output of the model at each stage and not for the kernel at that stage. Once filled, please call this method on a few sample images. Please leave a cooment of your observation? Elaborate generally in a text snippet below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59184f3",
   "metadata": {
    "id": "e59184f3"
   },
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56200546",
   "metadata": {
    "id": "56200546"
   },
   "outputs": [],
   "source": [
    "'''Sample an image from the test set'''\n",
    "#You may change the sampling code to sample an image as you desire.\n",
    "#Make sure to NOT move the sampling code to a different cell.\n",
    "img_batch, labels_batch = next(iter(testloader))\n",
    "img = img_batch[3]\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "classifier = Classifier(experiment_name, model, dataloaders, class_names, use_cuda=True)\n",
    "classifier.activations_on_input(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2000d",
   "metadata": {
    "id": "5cd2000d"
   },
   "source": [
    "What is your observation about early layers v. later layers? Please leave a commnet in a text snippet below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf7100",
   "metadata": {
    "id": "4bbf7100"
   },
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79130f77",
   "metadata": {
    "id": "79130f77"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "pytorch-gpu.1-8.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m69"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "12405b11f78b4f4bb19db5102461bb13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d02d457af42f4303b8ab195eacb434f1",
       "IPY_MODEL_90d3fd78a8ab47d49d861f1760be3a3e",
       "IPY_MODEL_fea008a7bd2b4372a6a733e1a6530799"
      ],
      "layout": "IPY_MODEL_1de732a5f6554fd7a5262acadbf6d398"
     }
    },
    "1de732a5f6554fd7a5262acadbf6d398": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d04f101ec9546afad0efd05d85a17f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44b96e0dc09a4db98e01f9a38da9ea76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7aed49d62b234bd29a47048ca82c6c86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e9a25d7d7d94353b19a34acf95d3bbc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90d3fd78a8ab47d49d861f1760be3a3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce3eb6de33a944c7897092304ef2d1ef",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2d04f101ec9546afad0efd05d85a17f9",
      "value": 170498071
     }
    },
    "b04888374a304257b9fa11f6689497a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce3eb6de33a944c7897092304ef2d1ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d02d457af42f4303b8ab195eacb434f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7aed49d62b234bd29a47048ca82c6c86",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_44b96e0dc09a4db98e01f9a38da9ea76",
      "value": ""
     }
    },
    "fea008a7bd2b4372a6a733e1a6530799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e9a25d7d7d94353b19a34acf95d3bbc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b04888374a304257b9fa11f6689497a5",
      "value": " 170499072/? [00:03&lt;00:00, 58778983.54it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}